# üîó VK Chart AI - –ú–û–î–ï–õ–¨ –ò–ù–¢–ï–ì–†–ê–¶–ò–ò LANG* STACK

**–í–µ—Ä—Å–∏—è**: 1.0  
**–î–∞—Ç–∞**: 2024-12-02  
**–°—Ç–∞—Ç—É—Å**: BLOCK B - Step B2 COMPLETED  
**–ê–≤—Ç–æ—Ä**: Claude (System Architect)

---

## üìã EXECUTIVE SUMMARY

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç **–ö–ê–ö –∏–º–µ–Ω–Ω–æ** LangChain —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞ –±—É–¥–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ VK Chart AI:
- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- Integration diagrams
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ use cases –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
- Implementation guide —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∫–æ–¥–∞

---

## 1. –ü–û–õ–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –° LANG* STACK

### 1.1 High-Level Integration Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         VK CHART AI SYSTEM                                   ‚îÇ
‚îÇ                      WITH FULL LANG* STACK                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LAYER 1: LARAVEL APPLICATION (Orchestrator)                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  UI/Dashboard ‚Üí Events ‚Üí Jobs ‚Üí RabbitMQ Publisher                          ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LAYER 2: RABBITMQ (Message Broker)                                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  [scoring.requests] [scoring.results] [messages.requests] [learning]        ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LAYER 3: PYTHON AI SERVICE (LangChain + LangGraph + LangServe)             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ LANGSERVE (API Layer)                                                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  FastAPI App with LangServe routes:                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ POST /scoring/invoke                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ POST /scoring/stream         ‚Üê Streaming progress                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ POST /scoring/batch                                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ GET  /scoring/playground     ‚Üê Interactive testing                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚îÇ                                                  ‚îÇ
‚îÇ                           ‚ñº                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ LANGGRAPH (Workflow Orchestration)                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ScoringWorkflow = StateGraph(ScoringState)                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   START ‚Üí [Normalize] ‚Üí [P1] ‚Üí [P1A] ‚Üí [P2] ‚Üí [P3]                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚Üì           ‚Üì                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         [Check Data]  [RAG]                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                           ‚Üì                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   [P4] ‚Üí [P5] ‚Üí [Check Intake] ‚Üí [P6] ‚Üí [Check Block]               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ           ‚Üì          ‚Üì               ‚Üì         ‚Üì                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ        [Tool]   [BLOCKED?]      [Parse]   [P6‚â§-40?]                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                     ‚Üì               ‚Üì         ‚Üì                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                   [END]         [P7] ‚Üí [Final Score] ‚Üí [END]         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                   ‚Üì                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                             [Checkpointing]                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                   ‚Üì                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                          [Human-in-the-loop]                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚îÇ                                                  ‚îÇ
‚îÇ                           ‚ñº                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ LANGCHAIN (Core Components)                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Agents     ‚îÇ  ‚îÇ    RAG      ‚îÇ  ‚îÇ   Tools     ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ Scoring   ‚îÇ  ‚îÇ ‚Ä¢ MongoDB   ‚îÇ  ‚îÇ ‚Ä¢ intake    ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ Message   ‚îÇ  ‚îÇ   Vector    ‚îÇ  ‚îÇ ‚Ä¢ distance  ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ Normalize ‚îÇ  ‚îÇ   Search    ‚îÇ  ‚îÇ ‚Ä¢ comments  ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ ‚Ä¢ Retriever ‚îÇ  ‚îÇ ‚Ä¢ eta       ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  LLMs       ‚îÇ  ‚îÇ Embeddings  ‚îÇ  ‚îÇ  Splitters  ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ Claude    ‚îÇ  ‚îÇ ‚Ä¢ OpenAI    ‚îÇ  ‚îÇ ‚Ä¢ Recursive ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚Ä¢ OpenAI    ‚îÇ  ‚îÇ   Ada-002   ‚îÇ  ‚îÇ ‚Ä¢ Character ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LAYER 4: LANGSMITH (Observability & Monitoring)                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Automatic Tracing ‚îÇ  ‚îÇ     Datasets       ‚îÇ  ‚îÇ    Evaluation      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ LLM calls       ‚îÇ  ‚îÇ  ‚Ä¢ Test cases      ‚îÇ  ‚îÇ  ‚Ä¢ Accuracy        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Tool calls      ‚îÇ  ‚îÇ  ‚Ä¢ Ground truth    ‚îÇ  ‚îÇ  ‚Ä¢ Latency         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ RAG queries     ‚îÇ  ‚îÇ  ‚Ä¢ Edge cases      ‚îÇ  ‚îÇ  ‚Ä¢ Cost            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Errors          ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Production        ‚îÇ  ‚îÇ    Debugging       ‚îÇ  ‚îÇ   Analytics        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Monitoring        ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ  ‚Ä¢ Trace viewer    ‚îÇ  ‚îÇ  ‚Ä¢ Cost tracking   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Requests/min    ‚îÇ  ‚îÇ  ‚Ä¢ Replay          ‚îÇ  ‚îÇ  ‚Ä¢ Token usage     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Avg latency     ‚îÇ  ‚îÇ  ‚Ä¢ Breakpoints     ‚îÇ  ‚îÇ  ‚Ä¢ Model usage     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Error rate      ‚îÇ  ‚îÇ  ‚Ä¢ Comparisons     ‚îÇ  ‚îÇ  ‚Ä¢ Trends          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LAYER 5: DATA STORES                                                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ  MongoDB Atlas   ‚îÇ  ‚îÇ  PostgreSQL      ‚îÇ  ‚îÇ    MySQL         ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ  (Checkpoints)   ‚îÇ  ‚îÇ    (Laravel)     ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Vectors       ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Scoring       ‚îÇ  ‚îÇ  ‚Ä¢ LangGraph     ‚îÇ  ‚îÇ  ‚Ä¢ Vessels       ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Learning      ‚îÇ  ‚îÇ    states        ‚îÇ  ‚îÇ  ‚Ä¢ Cargos        ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Messages      ‚îÇ  ‚îÇ  ‚Ä¢ Resume data   ‚îÇ  ‚îÇ  ‚Ä¢ Companies     ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. –î–ï–¢–ê–õ–ò–ó–ê–¶–ò–Ø –ü–û –ö–ê–ñ–î–û–ú–£ –ü–†–û–î–£–ö–¢–£

### 2.1 LangChain - Core Framework

**–†–æ–ª—å:** –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLMs

**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è:**

#### A. Agents
```python
from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain.prompts import ChatPromptTemplate

class ScoringAgent:
    """–ë–∞–∑–æ–≤—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —Å–∫–æ—Ä–∏–Ω–≥–∞ (–¥–æ LangGraph)"""
    
    def __init__(self, llm, tools):
        self.agent = create_structured_chat_agent(
            llm=llm,
            tools=tools,
            prompt=self._build_prompt()
        )
        self.executor = AgentExecutor(
            agent=self.agent,
            tools=tools,
            verbose=True
        )
```

#### B. RAG (Retrieval)
```python
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_openai import OpenAIEmbeddings

class KnowledgeRetriever:
    """RAG –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, mongo_collection):
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.vector_store = MongoDBAtlasVectorSearch(
            collection=mongo_collection,
            embedding=self.embeddings,
            index_name="vector_index"
        )
    
    async def retrieve_for_criterion(self, criterion: str, context: str):
        """Retrieve knowledge for P1-P7"""
        return await self.vector_store.asimilarity_search(
            query=context,
            k=5,
            pre_filter={"metadata.criterion": criterion}
        )
```

#### C. Tools
```python
from langchain.tools import tool

@tool
def calculate_intake(
    vessel_dwt: int,
    port_draft_limit: float,
    cargo_stowage_factor: float
) -> dict:
    """Calculate max cargo intake"""
    # Implementation
    return {"intake_weight": ..., "limiting_factor": ...}

@tool
def estimate_distance(port_from: str, port_to: str) -> dict:
    """Estimate sea distance"""
    # Implementation
    return {"distance_nm": ..., "estimated_days": ...}

@tool
def parse_comments(comments: str, comment_type: str) -> dict:
    """Parse structured data from comments"""
    # Implementation
    return {"preferences": [...], "technical_data": {...}}
```

#### D. LLM Integration
```python
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

# Primary LLM
claude = ChatAnthropic(
    model="claude-sonnet-4-20250514",
    temperature=0.7,
    max_tokens=2000
)

# Backup LLM
gpt4 = ChatOpenAI(
    model="gpt-4-turbo-preview",
    temperature=0.7
)
```

---

### 2.2 LangGraph - State Management & Workflows

**–†–æ–ª—å:** –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ state –∏ —Å–ª–æ–∂–Ω—ã–º–∏ multi-step workflows

**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è:**

#### A. Scoring Workflow

```python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.postgres import PostgresSaver
from typing import TypedDict, Annotated
from operator import add

class ScoringState(TypedDict):
    """State –¥–ª—è scoring workflow"""
    
    # Input
    vessel: dict
    cargo: dict
    company: dict
    
    # Intermediate state
    normalized_data: dict
    knowledge_context: dict
    
    # Scores
    scores: Annotated[dict, add]  # P1-P7 scores
    
    # Blocking
    is_blocked: bool
    block_reason: str
    
    # Output
    final_score: int
    reasoning: str
    strengths: list[str]
    weaknesses: list[str]
    
    # Metadata
    processing_steps: list[str]
    errors: list[str]


# Define nodes
def normalize_data(state: ScoringState) -> ScoringState:
    """Step 1: Normalize vessel/cargo data"""
    normalizer = PortNormalizer(...)
    state["normalized_data"] = {
        "open_port": await normalizer.normalize(state["vessel"]["open_area"]),
        "load_port": await normalizer.normalize(state["cargo"]["load_port"]),
        # ...
    }
    state["processing_steps"].append("normalize_data")
    return state


def calculate_p1_proximity(state: ScoringState) -> ScoringState:
    """Step 2: Calculate P1 Proximity"""
    
    # RAG: Get proximity rules
    retriever = KnowledgeRetriever(...)
    knowledge = await retriever.retrieve_for_criterion(
        "P1",
        f"proximity {state['vessel']['open_area']} to {state['cargo']['load_port']}"
    )
    
    # LLM: Calculate score
    prompt = f"""
Based on this knowledge:
{knowledge}

Calculate P1 Proximity score (0-20) for:
Vessel: {state["vessel"]["name"]} at {state["normalized_data"]["open_port"]}
Cargo: Loading at {state["normalized_data"]["load_port"]}

Return JSON: {{"score": ..., "reasoning": "..."}}
"""
    
    result = await claude.ainvoke(prompt)
    parsed = parse_json(result.content)
    
    state["scores"]["p1"] = {
        "score": parsed["score"],
        "max": 20,
        "reasoning": parsed["reasoning"]
    }
    state["processing_steps"].append("calculate_p1")
    return state


def calculate_p5_intake(state: ScoringState) -> ScoringState:
    """Step 3: Calculate P5 Intake"""
    
    # Use tool
    intake_result = calculate_intake_tool(
        vessel_dwt=state["vessel"]["dwt"],
        port_draft_limit=get_port_draft(state["cargo"]["load_port"]),
        cargo_stowage_factor=state["cargo"]["stowage_factor"]
    )
    
    # Check blocking condition
    if intake_result["intake_final"] < 0.9 * state["cargo"]["quantity"]:
        state["is_blocked"] = True
        state["block_reason"] = "Insufficient intake capacity"
        state["processing_steps"].append("calculate_p5_BLOCKED")
        return state
    
    # Calculate score based on intake percentage
    intake_pct = (intake_result["intake_final"] / state["cargo"]["quantity"]) * 100
    if intake_pct >= 100:
        score = 15
    elif intake_pct >= 95:
        score = 12
    else:
        score = 8
    
    state["scores"]["p5"] = {
        "score": score,
        "max": 15,
        "reasoning": f"Intake {intake_result['intake_final']}t vs cargo {state['cargo']['quantity']}t ({intake_pct:.1f}%)"
    }
    state["processing_steps"].append("calculate_p5")
    return state


def calculate_p6_openarea(state: ScoringState) -> ScoringState:
    """Step 4: Calculate P6 OpenArea Comments"""
    
    # Parse comments
    parsed = parse_comments_tool(
        state["vessel"]["comments"],
        comment_type="open_area"
    )
    
    # RAG: Get scoring rules
    retriever = KnowledgeRetriever(...)
    rules = await retriever.retrieve_for_criterion("P6", "open area comments scoring")
    
    # LLM: Evaluate alignment
    prompt = f"""
Based on these rules:
{rules}

Vessel comments: {state["vessel"]["comments"]}
Cargo: {state["cargo"]["type"]} from {state["cargo"]["load_region"]}

Score P6 OpenArea (-50 to +25): {{"score": ..., "reasoning": "..."}}
"""
    
    result = await claude.ainvoke(prompt)
    parsed_result = parse_json(result.content)
    
    state["scores"]["p6"] = {
        "score": parsed_result["score"],
        "range": (-50, 25),
        "reasoning": parsed_result["reasoning"]
    }
    
    # Check blocking condition
    if parsed_result["score"] <= -40:
        state["is_blocked"] = True
        state["block_reason"] = "Owner explicitly avoids this cargo/region (P6 ‚â§ -40)"
    
    state["processing_steps"].append("calculate_p6")
    return state


def calculate_final_score(state: ScoringState) -> ScoringState:
    """Final step: Calculate total score"""
    
    # Base score (P1, P1A, P2, P3, P4, P5, P7)
    base = sum([
        state["scores"]["p1"]["score"],
        state["scores"]["p1a"]["score"],
        state["scores"]["p2"]["score"],
        state["scores"]["p3"]["score"],
        state["scores"]["p4"]["score"],
        state["scores"]["p5"]["score"],
        state["scores"]["p7"]["score"]
    ])
    
    # Modifier (P6)
    modifier = state["scores"]["p6"]["score"]
    
    # Final score (clamped 0-100)
    state["final_score"] = max(0, min(100, base + modifier))
    
    # Classification
    if state["final_score"] >= 85:
        classification = "excellent"
    elif state["final_score"] >= 70:
        classification = "good"
    elif state["final_score"] >= 60:
        classification = "acceptable"
    elif state["final_score"] >= 50:
        classification = "marginal"
    else:
        classification = "poor"
    
    state["classification"] = classification
    state["processing_steps"].append("calculate_final_score")
    return state


# Conditional routing
def should_continue_after_p5(state: ScoringState) -> str:
    """Conditional edge after P5"""
    if state["is_blocked"]:
        return "blocked"
    return "continue"


def should_continue_after_p6(state: ScoringState) -> str:
    """Conditional edge after P6"""
    if state["is_blocked"]:
        return "blocked"
    return "continue"


# Build the graph
workflow = StateGraph(ScoringState)

# Add nodes
workflow.add_node("normalize", normalize_data)
workflow.add_node("p1", calculate_p1_proximity)
workflow.add_node("p1a", calculate_p1a_patterns)
workflow.add_node("p2", calculate_p2_regional)
workflow.add_node("p3", calculate_p3_cargo)
workflow.add_node("p4", calculate_p4_lastports)
workflow.add_node("p5", calculate_p5_intake)
workflow.add_node("p6", calculate_p6_openarea)
workflow.add_node("p7", calculate_p7_readiness)
workflow.add_node("final", calculate_final_score)

# Add edges
workflow.set_entry_point("normalize")
workflow.add_edge("normalize", "p1")
workflow.add_edge("p1", "p1a")
workflow.add_edge("p1a", "p2")
workflow.add_edge("p2", "p3")
workflow.add_edge("p3", "p4")
workflow.add_edge("p4", "p5")

# Conditional edges
workflow.add_conditional_edges(
    "p5",
    should_continue_after_p5,
    {
        "continue": "p6",
        "blocked": END
    }
)

workflow.add_conditional_edges(
    "p6",
    should_continue_after_p6,
    {
        "continue": "p7",
        "blocked": END
    }
)

workflow.add_edge("p7", "final")
workflow.add_edge("final", END)

# Compile with checkpointing
checkpointer = PostgresSaver.from_conn_string("postgresql://...")
app = workflow.compile(checkpointer=checkpointer)
```

#### B. Human-in-the-Loop Corrections

```python
# Execute workflow with checkpointing
config = {"configurable": {"thread_id": f"scoring_{vessel_id}_{cargo_id}"}}

result = await app.ainvoke(initial_state, config=config)

# User makes correction
# "P1 should be 8, not 15 - Libya to Black Sea not profitable for Handy"

# Get current state
current_state = await app.aget_state(config)

# Update P1 score
current_state.values["scores"]["p1"]["score"] = 8
current_state.values["scores"]["p1"]["reasoning"] = "Libya to Black Sea not profitable for Handysize"

# Resume from P1A node (recalculate downstream)
new_result = await app.ainvoke(
    None,  # Resume from checkpoint
    config={
        **config,
        "configurable": {
            **config["configurable"],
            "checkpoint_id": current_state.config["configurable"]["checkpoint_id"]
        }
    }
)
```

#### C. Visualization

```python
# Generate Mermaid diagram
from langgraph.graph import draw_mermaid

mermaid_code = draw_mermaid(app)
# ‚Üí –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç Mermaid –¥–∏–∞–≥—Ä–∞–º–º—É workflow

# Save to file
with open("scoring_workflow.mmd", "w") as f:
    f.write(mermaid_code)
```

---

### 2.3 LangServe - API Deployment

**–†–æ–ª—å:** –ë—ã—Å—Ç—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ production-ready API

**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è:**

#### A. Deploy LangGraph Workflow as API

```python
from fastapi import FastAPI
from langserve import add_routes

app = FastAPI(
    title="VK Chart AI Scoring API",
    version="1.0.0",
    description="Vessel-cargo scoring with LangChain"
)

# Add LangGraph workflow as API endpoint
add_routes(
    app,
    app=scoring_workflow,  # The compiled LangGraph app
    path="/scoring",
    enabled_endpoints=["invoke", "stream", "batch"],
    playground_type="default"
)

# Automatically creates:
# POST /scoring/invoke       - Single request
# POST /scoring/stream       - Streaming response
# POST /scoring/batch        - Batch processing
# GET  /scoring/playground   - Interactive playground
```

#### B. Streaming Progress

```python
# Client-side (Laravel/React)
const response = await fetch("/scoring/stream", {
    method: "POST",
    headers: {"Content-Type": "application/json"},
    body: JSON.stringify({
        input: {vessel: ..., cargo: ...},
        config: {configurable: {thread_id: "..."}}
    })
});

const reader = response.body.getReader();
while (true) {
    const {done, value} = await reader.read();
    if (done) break;
    
    // Parse streaming events
    const event = JSON.parse(value);
    
    if (event.event === "on_chain_stream") {
        // Update UI with progress
        updateProgress(event.data.chunk.processing_steps);
    }
}

// UI shows:
// ‚úì Normalizing data...
// ‚úì Calculating P1 Proximity... (15/20)
// ‚úì Calculating P1A Patterns... (12/15)
// ‚úì Calculating P2 Regional... (10/15)
// ...
```

#### C. Interactive Playground

```python
# Just navigate to:
# http://localhost:8000/scoring/playground

# Interactive UI automatically generated:
# - Input form for ScoringState
# - Execute button
# - Response viewer
# - Streaming progress
# - Error display
```

---

### 2.4 LangSmith - Observability & Monitoring

**–†–æ–ª—å:** Debugging, testing, evaluation, monitoring

**–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è:**

#### A. Automatic Tracing

```python
import os

# Setup (one-time)
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "lsv2_pt_..."
os.environ["LANGSMITH_PROJECT"] = "vkchart-ai-scoring"

# That's it! All LangChain/LangGraph calls are now traced
```

**What gets traced automatically:**
- Every LLM call (prompt, response, tokens, cost)
- Every tool call (input, output, duration)
- Every RAG query (query, retrieved docs, relevance)
- Errors and exceptions
- Full execution graph

#### B. Datasets & Evaluation

```python
from langsmith import Client

client = Client()

# Create dataset
dataset = client.create_dataset(
    dataset_name="vkchart-scoring-test-cases",
    description="20 test cases for scoring accuracy"
)

# Add test cases
test_cases = [
    {
        "input": {
            "vessel": {"name": "TEST VESSEL 1", "dwt": 55000, ...},
            "cargo": {"type": "grain", "quantity": 50000, ...}
        },
        "expected_output": {
            "final_score": 78,
            "scores": {"p1": 15, "p2": 10, ...}
        }
    },
    # ... 19 more cases
]

for case in test_cases:
    client.create_example(
        dataset_id=dataset.id,
        inputs=case["input"],
        outputs=case["expected_output"]
    )
```

```python
from langsmith.evaluation import evaluate

# Define evaluator
def scoring_accuracy(run, example):
    """Check if score is within ¬±5 of expected"""
    actual = run.outputs["final_score"]
    expected = example.outputs["expected_output"]["final_score"]
    
    return {
        "key": "accuracy",
        "score": abs(actual - expected) <= 5
    }

def p1_accuracy(run, example):
    """Check P1 specifically"""
    actual = run.outputs["scores"]["p1"]["score"]
    expected = example.outputs["expected_output"]["scores"]["p1"]
    
    return {
        "key": "p1_accuracy",
        "score": abs(actual - expected) <= 2
    }

# Run evaluation
results = evaluate(
    lambda inputs: scoring_workflow.invoke(inputs),
    data="vkchart-scoring-test-cases",
    evaluators=[scoring_accuracy, p1_accuracy],
    experiment_prefix="scoring-v1"
)

# Results:
# Accuracy: 87% (17/20 within ¬±5)
# P1 Accuracy: 90% (18/20 within ¬±2)
```

#### C. Production Monitoring

```python
# Dashboard automatically shows:
# - Requests per minute
# - Average latency
# - Error rate
# - Cost per request
# - Token usage trends

# Alerts setup
client.create_alert(
    name="High Error Rate",
    condition="error_rate > 0.05",  # >5% errors
    notification_channels=["email", "slack"]
)

client.create_alert(
    name="High Latency",
    condition="p95_latency > 10000",  # >10 seconds
    notification_channels=["email"]
)
```

#### D. Debugging Workflow

```python
# Scenario: P1 score is wrong (15 instead of expected 8)

# 1. Find the trace in LangSmith UI
# 2. View full execution graph:
#    normalize ‚Üí p1 ‚Üí p1a ‚Üí p2 ‚Üí ...
# 3. Click on "p1" node
# 4. See:
#    - Input state
#    - RAG query: "proximity Libya to Black Sea"
#    - Retrieved documents (with relevance scores)
#    - LLM prompt sent to Claude
#    - LLM response: {"score": 15, "reasoning": "..."}
#    - Output state
# 5. Identify issue:
#    - RAG didn't retrieve document about "Handy not profitable Libya‚ÜíBlackSea"
#    - Need to add this to knowledge base or adjust embedding

# 6. Fix and re-run evaluation
```

---

## 3. –ê–†–•–ò–¢–ï–ö–¢–£–†–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´

### 3.1 Pattern: Sequential Workflow with Conditional Routing

**Use Case:** Scoring P1-P7 —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞–º–∏

```python
# LangGraph pattern
workflow = StateGraph(State)
workflow.add_node("step1", func1)
workflow.add_node("step2", func2)
workflow.add_conditional_edges(
    "step2",
    condition_func,
    {"continue": "step3", "stop": END}
)
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –í–∏–∑—É–∞–ª—å–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ flow
- –õ–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ —à–∞–≥–∏
- Conditional routing declarative

---

### 3.2 Pattern: Human-in-the-Loop Corrections

**Use Case:** –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Å–∫–æ—Ä–∏–Ω–≥

```python
# Execute with checkpointing
result = await app.ainvoke(input, config={"configurable": {"thread_id": id}})

# User corrects
state = await app.aget_state(config)
state.values["scores"]["p1"] = corrected_value

# Resume workflow
new_result = await app.ainvoke(None, config=resume_config)
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ù–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å –≤—Å—ë —Å –Ω—É–ª—è
- State —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- –ú–æ–∂–Ω–æ –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å —Å –ª—é–±–æ–≥–æ checkpoint

---

### 3.3 Pattern: RAG-Enhanced Agent

**Use Case:** –ö–∞–∂–¥—ã–π P1-P7 –∫—Ä–∏—Ç–µ—Ä–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç RAG

```python
def calculate_criterion(state: State) -> State:
    # 1. RAG retrieval
    knowledge = await retriever.retrieve(criterion, context)
    
    # 2. LLM —Å knowledge context
    prompt = f"Based on: {knowledge}\nCalculate: {criterion}"
    result = await llm.ainvoke(prompt)
    
    # 3. Update state
    state["scores"][criterion] = parse(result)
    return state
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ö–∞–∂–¥—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç relevant knowledge
- LLM –Ω–µ hallucinate –ø—Ä–∞–≤–∏–ª–∞
- –ú–æ–∂–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å knowledge base –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞

---

### 3.4 Pattern: Streaming Progress

**Use Case:** UI –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å —Å–∫–æ—Ä–∏–Ω–≥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏

```python
# Server: LangServe automatic streaming
add_routes(app, workflow, path="/scoring", enabled_endpoints=["stream"])

# Client: Consume stream
async for chunk in stream_response:
    if chunk["event"] == "on_chain_stream":
        step = chunk["data"]["chunk"]["processing_steps"][-1]
        update_ui(step)  # "Calculating P1...", "Calculating P2...", etc.
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- Better UX (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–∏–¥–∏—Ç progress)
- –ú–æ–∂–Ω–æ –æ—Ç–º–µ–Ω–∏—Ç—å long-running requests
- –õ–µ–≥–∫–æ debug (–≤–∏–¥–Ω–æ –≥–¥–µ –∑–∞—Å—Ç—Ä—è–ª–æ)

---

### 3.5 Pattern: Evaluation Loop

**Use Case:** Regression testing –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π

```python
# 1. Create dataset with 20 test cases
# 2. Run evaluation
results = evaluate(workflow, data="test-cases", evaluators=[...])

# 3. Make changes to prompts/knowledge
# 4. Re-run evaluation ‚Üí compare results
# 5. If accuracy improved ‚Üí deploy
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- Prevent regression
- Measure progress (accuracy 80% ‚Üí 87%)
- A/B testing prompts

---

## 4. IMPLEMENTATION GUIDE

### 4.1 Phase 1: LangSmith Setup (Week 2-3)

**Duration:** 2-4 hours

#### Step 1: Sign up & Configure

```bash
# 1. Sign up at langsmith.com
# 2. Create API key
# 3. Add to .env

LANGSMITH_TRACING=true
LANGSMITH_API_KEY=lsv2_pt_xxx
LANGSMITH_PROJECT=vkchart-ai-scoring
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
```

#### Step 2: Verify Tracing

```python
# Test script
import os
from langchain_anthropic import ChatAnthropic

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "..."

llm = ChatAnthropic(model="claude-sonnet-4-20250514")
result = llm.invoke("Test message")

# Check LangSmith UI ‚Üí Should see trace
```

#### Step 3: Create Dataset

```python
from langsmith import Client

client = Client()
dataset = client.create_dataset("vkchart-scoring-test-cases")

# Add 20 test cases
# Can import from CSV or add programmatically
```

#### Step 4: Setup Evaluation

```python
def scoring_accuracy(run, example):
    actual = run.outputs["final_score"]
    expected = example.outputs["final_score"]
    return {"score": abs(actual - expected) <= 5}

results = evaluate(
    scoring_agent,
    data="vkchart-scoring-test-cases",
    evaluators=[scoring_accuracy]
)
```

#### Step 5: Production Monitoring

```python
# Setup alerts in LangSmith UI:
# - Error rate > 5%
# - P95 latency > 10s
# - Cost per day > $50
```

---

### 4.2 Phase 2: LangGraph Refactoring (Week 4)

**Duration:** 1-2 days

#### Step 1: Design State Schema

```python
from typing import TypedDict, Annotated
from operator import add

class ScoringState(TypedDict):
    # Input
    vessel: dict
    cargo: dict
    company: dict
    
    # Scores (accumulated)
    scores: Annotated[dict, add]
    
    # Blocking
    is_blocked: bool
    block_reason: str
    
    # Output
    final_score: int
    reasoning: str
```

#### Step 2: Convert Each Criterion to Node

```python
def calculate_p1(state: ScoringState) -> ScoringState:
    # Existing logic from ScoringAgent
    # Move into node function
    state["scores"]["p1"] = calculate_p1_logic(state)
    return state
```

#### Step 3: Build Graph

```python
from langgraph.graph import StateGraph, END

workflow = StateGraph(ScoringState)
workflow.add_node("p1", calculate_p1)
workflow.add_node("p2", calculate_p2)
# ...
workflow.add_edge("p1", "p2")
# ...
app = workflow.compile()
```

#### Step 4: Add Checkpointing

```python
from langgraph.checkpoint.postgres import PostgresSaver

checkpointer = PostgresSaver.from_conn_string(
    "postgresql://user:pass@localhost/vkchart"
)
app = workflow.compile(checkpointer=checkpointer)
```

#### Step 5: Testing

```python
# Test with 20 cases
for case in test_cases:
    result = await app.ainvoke(case["input"])
    assert result["final_score"] == case["expected_output"]
```

---

### 4.3 Phase 3: LangServe Integration (Optional)

**Duration:** 2-4 hours

#### Step 1: Add LangServe

```bash
pip install langserve[all]
```

#### Step 2: Update FastAPI

```python
from langserve import add_routes

add_routes(
    app,
    app=scoring_workflow,
    path="/scoring"
)
```

#### Step 3: Test Endpoints

```bash
# Invoke
curl -X POST http://localhost:8000/scoring/invoke \
  -H "Content-Type: application/json" \
  -d '{"input": {...}}'

# Stream
curl -X POST http://localhost:8000/scoring/stream \
  -H "Content-Type: application/json" \
  -d '{"input": {...}}'
```

#### Step 4: Update Laravel Client

```php
// Support streaming
$response = Http::asStream()
    ->post('http://ai-service:8000/scoring/stream', $data);

foreach ($response as $chunk) {
    event(new ScoringProgressEvent($chunk));
}
```

---

## 5. BENEFITS SUMMARY

### 5.1 LangSmith Benefits

| Benefit | Impact | Example |
|---------|--------|---------|
| **Debugging** | HIGH | "Why P1=15 not 8?" ‚Üí See full trace, LLM prompt, RAG results |
| **Evaluation** | HIGH | Test 20 cases ‚Üí Measure accuracy 87% ‚Üí Track improvement |
| **Cost Tracking** | MEDIUM | See daily Claude API cost ‚Üí Optimize expensive calls |
| **Production Monitoring** | HIGH | Alert if error rate >5% ‚Üí Catch issues before users |

### 5.2 LangGraph Benefits

| Benefit | Impact | Example |
|---------|--------|---------|
| **Visualization** | HIGH | See P1‚ÜíP2‚ÜíP3 flow as diagram ‚Üí Easy to understand |
| **Maintainability** | HIGH | Add P8 criterion ‚Üí Just add new node, no refactor |
| **Human-in-loop** | HIGH | User corrects P1 ‚Üí Resume from checkpoint, recalc P2-P7 |
| **Conditional Logic** | MEDIUM | If P6‚â§-40 ‚Üí Skip P7 ‚Üí Declarative, not imperative |

### 5.3 LangServe Benefits

| Benefit | Impact | Example |
|---------|--------|---------|
| **Playground** | MEDIUM | Test API without writing client code ‚Üí Faster dev |
| **Streaming** | MEDIUM | UI shows "Calculating P1... P2..." ‚Üí Better UX |
| **Auto Docs** | LOW | OpenAPI spec generated ‚Üí API docs for free |

---

## 6. NEXT STEPS

1. ‚úÖ **–í–∏—Ç–∞–ª–∏–π review —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç** (LANG_STACK_INTEGRATION_MODEL.md)
2. ‚úÖ **–û–¥–æ–±—Ä–∏—Ç—å –ø–ª–∞–Ω –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏**
3. üöÄ **–ù–∞—á–∞—Ç—å Phase 1**: LangSmith setup (2-4 hours)
4. üîÑ **–ü–µ—Ä–µ–π—Ç–∏ –∫ B3**: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Å—Ö–µ–º

---

## APPENDIX: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Current vs Proposed

### Current Architecture (Without LangGraph/LangSmith)

```python
# –ò–º–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –∫–æ–¥
async def score(snapshot):
    scores = {}
    
    # P1
    p1_result = await calculate_p1(snapshot)
    scores["p1"] = p1_result
    
    # P5
    if intake < threshold:
        return {"blocked": True}
    scores["p5"] = p5_result
    
    # P6
    p6 = await calculate_p6(snapshot)
    if p6 <= -40:
        return {"blocked": True}
    
    # ... etc
    return {"scores": scores}
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- ‚ùå –ù–µ—Ç visualization
- ‚ùå –ù–µ—Ç checkpointing
- ‚ùå –¢—Ä—É–¥–Ω–æ debug (verbose logs)
- ‚ùå –ù–µ—Ç evaluation framework
- ‚ùå –ù–µ—Ç production monitoring

---

### Proposed Architecture (With LangGraph/LangSmith)

```python
# –î–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω—ã–π –≥—Ä–∞—Ñ + automatic tracing
workflow = StateGraph(ScoringState)
workflow.add_node("p1", calculate_p1)
workflow.add_node("p5", calculate_p5)
workflow.add_conditional_edges("p5", should_continue, {...})
app = workflow.compile(checkpointer=checkpointer)

# Automatic LangSmith tracing
result = await app.ainvoke(input, config={"thread_id": "..."})
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ Visual diagram –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- ‚úÖ Checkpointing –≤—Å—Ç—Ä–æ–µ–Ω
- ‚úÖ Full trace –≤ LangSmith UI
- ‚úÖ Evaluation framework ready
- ‚úÖ Production monitoring out-of-box

---

**STATUS:** ‚úÖ BLOCK B - STEP B2 COMPLETED  
**NEXT:** B3 - –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Å—Ö–µ–º —Å Lang* –±–ª–æ–∫–∞–º–∏
